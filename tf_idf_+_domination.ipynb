{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf-idf + domination.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UT0KZRG-16C"
      },
      "source": [
        "#Dataset 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri3yIpqnwa9-",
        "outputId": "f4a3b5b4-fdbb-4d5b-8a97-930c7a6f68ce"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdHpB47u8_g3"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "3qgvUqIUCTIa",
        "outputId": "fef336e9-8b38-4344-eb2c-39498e0a00e2"
      },
      "source": [
        "file_1 = '/content/gdrive/MyDrive/summarisation_dataset/essg_dataframe.pkl'\n",
        "data_frame = pd.read_pickle(file_1)\n",
        "data_frame.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>story</th>\n",
              "      <th>summary</th>\n",
              "      <th>filename</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In the 17th century, Isaac Newton discovered t...</td>\n",
              "      <td>[[comparing newton's observation of prismatic ...</td>\n",
              "      <td>10.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>﻿A sea is a large body of salt water that is s...</td>\n",
              "      <td>[[owing to the present state of continental dr...</td>\n",
              "      <td>100.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>﻿Nazi Germany and the Third Reich (German: Dri...</td>\n",
              "      <td>[[nazi germany and the third reich (german: dr...</td>\n",
              "      <td>1.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A rainbow is a meteorological phenomenon that ...</td>\n",
              "      <td>[[rainbows can be full circles; however, the a...</td>\n",
              "      <td>11.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>﻿Shiva  meaning \"The Auspicious One\", also kno...</td>\n",
              "      <td>[[he is the parabrahman within shaivism, one o...</td>\n",
              "      <td>26.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               story  ... filename\n",
              "0  In the 17th century, Isaac Newton discovered t...  ...   10.txt\n",
              "1  ﻿A sea is a large body of salt water that is s...  ...  100.txt\n",
              "2  ﻿Nazi Germany and the Third Reich (German: Dri...  ...    1.txt\n",
              "3  A rainbow is a meteorological phenomenon that ...  ...   11.txt\n",
              "4  ﻿Shiva  meaning \"The Auspicious One\", also kno...  ...   26.txt\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb3ZstzIDIiS"
      },
      "source": [
        "data_frame.to_csv('essg_dataframe.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "KlzAzjUr791a",
        "outputId": "ca3ad12b-753c-4f1e-f654-cf017470f618"
      },
      "source": [
        "file_path = '/content/gdrive/MyDrive/summarisation_dataset/tfidf_keywords_dataframe.pkl'\n",
        "df_keywords = pd.read_pickle(file_path)\n",
        "df_keywords.head()  # original df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>story</th>\n",
              "      <th>top_keywords</th>\n",
              "      <th>summmary</th>\n",
              "      <th>filename</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In the 17th century, Isaac Newton discovered t...</td>\n",
              "      <td>[color, spectrum, blue, indigo, newton, light,...</td>\n",
              "      <td>[[comparing newton's observation of prismatic ...</td>\n",
              "      <td>10.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>﻿A sea is a large body of salt water that is s...</td>\n",
              "      <td>[sea, ocean, water, land, produc, cycl, whale,...</td>\n",
              "      <td>[[owing to the present state of continental dr...</td>\n",
              "      <td>100.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>﻿Nazi Germany and the Third Reich (German: Dri...</td>\n",
              "      <td>[hitler, germani, reich, nazi, german, power, ...</td>\n",
              "      <td>[[nazi germany and the third reich (german: dr...</td>\n",
              "      <td>1.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A rainbow is a meteorological phenomenon that ...</td>\n",
              "      <td>[rainbow, arc, droplet, refract, sky, inner, l...</td>\n",
              "      <td>[[rainbows can be full circles; however, the a...</td>\n",
              "      <td>11.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>﻿Shiva  meaning \"The Auspicious One\", also kno...</td>\n",
              "      <td>[shiva, god, hinduism, benevol, worship, depic...</td>\n",
              "      <td>[[he is the parabrahman within shaivism, one o...</td>\n",
              "      <td>26.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               story  ... filename\n",
              "0  In the 17th century, Isaac Newton discovered t...  ...   10.txt\n",
              "1  ﻿A sea is a large body of salt water that is s...  ...  100.txt\n",
              "2  ﻿Nazi Germany and the Third Reich (German: Dri...  ...    1.txt\n",
              "3  A rainbow is a meteorological phenomenon that ...  ...   11.txt\n",
              "4  ﻿Shiva  meaning \"The Auspicious One\", also kno...  ...   26.txt\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwreQJz4_gHy"
      },
      "source": [
        "file_path = '/content/gdrive/MyDrive/summarisation_dataset/tfidf_keywords_dataframe.pkl'\n",
        "df1 = pd.read_pickle(file_path)  # use for data cleaning part "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR0wk-R2_DaG"
      },
      "source": [
        "#Data Cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa3KL6f8895D"
      },
      "source": [
        "# Data Cleaning\n",
        "\n",
        "# separate sentences using spacy -> primitive cleaning -> stop word removal -> lemmatization -> stemming\n",
        "# update story column with a list of sentences "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCNgaM8LAtVt"
      },
      "source": [
        "# separate sentences using spacy \n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def separate_sentences(paragraph):\n",
        "  sent_list = []\n",
        "  paragraph = nlp(paragraph)\n",
        "  for sents in paragraph.sents:\n",
        "    sent_list.append(sents.text)\n",
        "  return sent_list\n",
        "\n",
        "# separate sentences using regex\n",
        "\n",
        "import re\n",
        "\n",
        "def sent_separation(par):\n",
        "  par = par.replace('\\n',\" \").replace('\\r', '')\n",
        "  #par = par.replace('(',\"\").replace(')','')\n",
        "  par = par.replace('\"',' ')\n",
        "  #m = re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', par)\n",
        "  #(?<![A-Z][a-z]\\.)(?<=\\.|\\?) +(?=[A-Z0-9])\n",
        "  m = re.split('(?<![A-Z][a-z]\\.)(?<=\\.|\\?) +(?=[A-Z0-9])', par)\n",
        "  sentences=[]\n",
        "  for i in m:\n",
        "    sentences.append(i)\n",
        "  return sentences\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8Q7jtprA6vR"
      },
      "source": [
        "df1['story'] = df1['story'].apply(sent_separation) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4WIf-d6D9Ip",
        "outputId": "3b807ad4-aa72-44b7-b278-79e8cf135fde"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.55)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wLGNSW2F1Kj"
      },
      "source": [
        "PUNCTUATION = \"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fHT65w7CnN-"
      },
      "source": [
        "# primitive cleaning \n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import contractions\n",
        "\n",
        "TAG_RE = re.compile(r'<[^>]+>')  # remove html tags\n",
        "\n",
        "def remove_tags(text):\n",
        "  return TAG_RE.sub('',text)\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)  # I'll -> I  will \n",
        "\n",
        "def primitive_cleaning_sentence(sen):\n",
        "  sentence = remove_tags(sen)\n",
        "  sentence = sentence.translate(sentence.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "\n",
        "  # sentence = sentence.replace(\".\",\"\")  \n",
        "  # not required, done already\n",
        "\n",
        "  sentence = expand_contractions(sentence)\n",
        "  sentence = re.sub('[^a-zA-Z]', ' ', sentence) # 17th -> after removal becomes 'th' ?\n",
        "  sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "  sentence = re.sub(r'\\s+', ' ', sentence)  #  one or more whitespace characters\n",
        "  sentence = sentence.lower()\n",
        "\n",
        "  # Removing punctuation from the ssentence\n",
        "  sentence = \"\".join([c for c in sentence if c not in PUNCTUATION])  \n",
        "\n",
        "  return sentence\n",
        "\n",
        "def primitive_clean_row(sent_list):\n",
        "  for i in range(len(sent_list)):\n",
        "    sent_list[i] = primitive_cleaning_sentence(sent_list[i])\n",
        "  return sent_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pxz8kK66D3UF"
      },
      "source": [
        "df1['story'] = df1['story'].apply(primitive_clean_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltFU9LgaGh3X",
        "outputId": "2bffc953-545c-489d-acd3-6e382642c7fe"
      },
      "source": [
        "# Stop word removal\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def stop_word_removal_sent(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  new_sent = \"\"\n",
        "\n",
        "  for w in word_tokens:\n",
        "    if w.lower() not in stop_words:\n",
        "      if new_sent == \"\":\n",
        "        new_sent += w\n",
        "      else:\n",
        "        new_sent += \" \" + w \n",
        "\n",
        "  return new_sent  \n",
        "\n",
        "def stop_word_removal_row(sent_list):\n",
        "  for i in range(len(sent_list)):\n",
        "    sent_list[i] = stop_word_removal_sent(sent_list[i])\n",
        "  return sent_list\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n5lDI7-HKcz"
      },
      "source": [
        "df1['story'] = df1['story'].apply(stop_word_removal_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-LpPYPIHh7q"
      },
      "source": [
        "# Lemmatization and Stemming\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def lemmatize_a_sent(text):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  word_list = nltk.word_tokenize(text)  # Tokenize: Split the sentence into words\n",
        "  text = ' '.join([lemmatizer.lemmatize(w) for w in word_list])  # Lemmatize list of words and join\n",
        "  return text \n",
        "\n",
        "def lemmatize_a_row(sent_list):\n",
        "  for i in range(len(sent_list)):\n",
        "    sent_list[i] = lemmatize_a_sent(sent_list[i])\n",
        "  return sent_list\n",
        "\n",
        "def stem_a_sent(text):\n",
        "  ps = PorterStemmer()\n",
        "  word_list = nltk.word_tokenize(text)  # Tokenize: Split the sentence into words\n",
        "  text = ' '.join([ps.stem(w) for w in word_list])  # stem list of words and join\n",
        "  return text\n",
        "\n",
        "def stem_a_row(sent_list):\n",
        "  for i in range(len(sent_list)):\n",
        "    sent_list[i] = stem_a_sent(sent_list[i])\n",
        "  return sent_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNisiDopIOTv"
      },
      "source": [
        "df1['story'] = df1['story'].apply(lemmatize_a_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uwh6t_kjIind"
      },
      "source": [
        "df1['story'] = df1['story'].apply(stem_a_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr-lFg4UI18E"
      },
      "source": [
        "stemming  <br>\n",
        "\n",
        "otherwise -> otherwise,\n",
        "edge -> edg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLhmXVl3_HUh"
      },
      "source": [
        "# Hypergraph Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5whKTB_0Nfj8"
      },
      "source": [
        "file_path = '/content/gdrive/MyDrive/summarisation_dataset/tfidf_keywords_dataframe.pkl'\n",
        "df2 = pd.read_pickle(file_path)  # use for hypergraph part \n",
        "# 'story' column of df2 will have the hypergraph dictionaries for all the stories"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pElEXBzgJOAc"
      },
      "source": [
        "# find sentences that contain a particular keyword -> convert into a dictionary \n",
        "# will have to check this for n-grams ?\n",
        "\n",
        "# hypergraph -> dictionay -> \n",
        "# key -> keyword \n",
        "# value -> list of indices sentences containing that keyword \n",
        "\n",
        "import re\n",
        "\n",
        "# using regex\n",
        "\n",
        "# def create_hypergraph_row(sent_list, keyword_array):\n",
        "#   txt = ' . '.join(str(x) for x in sent_list)  # one paragraph\n",
        "#   hypergraph_dict = dict()  # hypergraph for one document \n",
        "#   for word in keyword_array:\n",
        "#       exprn = r'([^.]*'+ word +'[^.]*)'          # required sentence \n",
        "#       hypergraph_dict[word] = re.findall(exprn, txt)  # find all sentences in the paragraph containing this keyword.\n",
        "#   return hypergraph_dict\n",
        "\n",
        "# simply check if the word is preseent in the sentence\n",
        "\n",
        "def create_hypergraph_row(sent_list, keyword_array):\n",
        "  hypergraph_dict = dict()  # hypergraph for one document\n",
        "  for i in range(len(keyword_array)):\n",
        "    hypergraph_dict[keyword_array[i]] = []  # initialize a list of sentences\n",
        "    for j in range(len(sent_list)):\n",
        "      if keyword_array[i] in sent_list[j]:\n",
        "        hypergraph_dict[keyword_array[i]].append(j)  # add the index of the sentence\n",
        "  return hypergraph_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-kMihigZqBn"
      },
      "source": [
        "df2['story'] = df1.apply(lambda x: create_hypergraph_row(x.story, x.top_keywords), axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6G9oMpq_PmP"
      },
      "source": [
        "#Domination\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9oWINCYhKBT"
      },
      "source": [
        "file_path = '/content/gdrive/MyDrive/summarisation_dataset/tfidf_keywords_dataframe.pkl'\n",
        "df3 = pd.read_pickle(file_path)  # use for hypergraph part \n",
        "# 'story' column of df2 will have the hypergraph dictionaries for all the stories"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUqv3WhEqFXz"
      },
      "source": [
        "def clean_sent_for_summary(sent):\n",
        "  sentence = remove_tags(sent)  # remove html tags\n",
        "  sentence = sentence.translate(sentence.maketrans(\"\\n\\t\\r\", \"   \"))  \n",
        "  sentence = expand_contractions(sentence)\n",
        "  sentence = re.sub(r'\\s+', ' ', sentence)  #  one or more whitespace characters\n",
        "  return sentence\n",
        "\n",
        "def clean_row_for_summary(sent_list):\n",
        "  for i in range(len(sent_list)):\n",
        "    sent_list[i] = clean_sent_for_summary(sent_list[i])\n",
        "  return sent_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTLOTlwvo6ib"
      },
      "source": [
        "file_path = '/content/gdrive/MyDrive/summarisation_dataset/tfidf_keywords_dataframe.pkl'\n",
        "df4 = pd.read_pickle(file_path)  # only for separating sentences from the original paragraph\n",
        "# sentences from this dataframe will be put in the hypothesized summary \n",
        "# story column will be a list of separated sentences\n",
        "\n",
        "df4['story'] = df4['story'].apply(sent_separation)  # function updated\n",
        "df4['story'] = df4['story'].apply(clean_row_for_summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R04wDHVmrvcP"
      },
      "source": [
        "# Newton\\'s -> text cleaning -> handling this '\\' ? "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnwKGBUid15V",
        "outputId": "f4cf28ee-fc61-44b5-c780-4cbc32ba7d74"
      },
      "source": [
        "!pip install hypernetx\n",
        "!pip install celluloid\n",
        "!pip install igraph"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hypernetx in /usr/local/lib/python3.7/dist-packages (1.2)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from hypernetx) (1.1.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from hypernetx) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from hypernetx) (0.22.2.post1)\n",
            "Requirement already satisfied: networkx<3.0,>=2.2 in /usr/local/lib/python3.7/dist-packages (from hypernetx) (2.6.3)\n",
            "Requirement already satisfied: matplotlib>3.0 in /usr/local/lib/python3.7/dist-packages (from hypernetx) (3.2.2)\n",
            "Requirement already satisfied: scipy<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from hypernetx) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>3.0->hypernetx) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>3.0->hypernetx) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>3.0->hypernetx) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>3.0->hypernetx) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>3.0->hypernetx) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->hypernetx) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->hypernetx) (1.0.1)\n",
            "Requirement already satisfied: celluloid in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from celluloid) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->celluloid) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->celluloid) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->celluloid) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->celluloid) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->celluloid) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->celluloid) (1.15.0)\n",
            "Requirement already satisfied: igraph in /usr/local/lib/python3.7/dist-packages (0.9.8)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.7/dist-packages (from igraph) (1.6.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ij1RCJQdN3u"
      },
      "source": [
        "import networkx as nx\n",
        "import hypernetx as hnx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoKAcWASknLm"
      },
      "source": [
        "# function to find the set in the list of dominating sets with maximum number of elements\n",
        "# return the first list with maximum number of elements\n",
        "\n",
        "def find_list_with_max_elements(dom_list):\n",
        "  no_of_elements = []  # find number of elements in every set \n",
        "  for i in range(len(dom_list)):\n",
        "    no_of_elements.append(len(dom_list[i]))  # length of every dominating set\n",
        "  max_ele = max(no_of_elements)\n",
        "\n",
        "  for i in range(len(dom_list)):\n",
        "    if len(dom_list[i]) == max_ele:\n",
        "      return i \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FY_K1lZRaxM5"
      },
      "source": [
        "# list of dominating set (original sentences) go in the 'story' column\n",
        "# original sentence in the story should be added in the summary \n",
        "\n",
        "def domination_row(keyword_sent_dict):\n",
        "  # finding neighbors\n",
        "  h = hnx.Hypergraph(keyword_sent_dict)\n",
        "  li = []\n",
        "  for i in list(h.nodes):\n",
        "      li.append(h.neighbors(i))\n",
        "\n",
        "  # Mapping nodes to numbers\n",
        "  mapp = {}\n",
        "  c = 0\n",
        "  for i in list(h.nodes):\n",
        "      mapp[i] = c\n",
        "      c = c + 1\n",
        "\n",
        "  # GFG code\n",
        "  # s = []\n",
        "  ver = len(li)\n",
        "  # visit = np.zeros(ver)\n",
        "\n",
        "  deg = {}\n",
        "  for i in range(len(li)):\n",
        "      deg[i] = len(li[i])\n",
        "\n",
        "  deg = dict(sorted(deg.items(), key = lambda item: item[1], reverse = True))\n",
        "\n",
        "  dom_li = []\n",
        "\n",
        "  # for i in deg.keys():\n",
        "  #     if(visit[i] == 0):\n",
        "  #         s.append(i)\n",
        "  #         visit[i] = 1\n",
        "  #         for j in range(len(li[i])):\n",
        "  #             if(visit[mapp[li[i][j]]] == 0):\n",
        "  #                 visit[mapp[li[i][j]]] = 1\n",
        "\n",
        "  for di in range(ver):\n",
        "    visit = np.zeros(ver)\n",
        "    s = []\n",
        "    \n",
        "    k = list(deg.keys())[0]\n",
        "    v = list(deg.values())[0]\n",
        "    for i in deg.keys():\n",
        "        if(visit[i] == 0):\n",
        "            s.append(i)\n",
        "            visit[i]=1\n",
        "            for j in range(len(li[i])):\n",
        "                if(visit[mapp[li[i][j]]]==0):\n",
        "                    visit[mapp[li[i][j]]]=1\n",
        "    deg.pop(list(deg.keys())[0])\n",
        "    deg[k] = v\n",
        "    dom_li.append(s)\n",
        "\n",
        "  return sorted(dom_li[find_list_with_max_elements(dom_li)])  # returns the dominating set with the maximum elements"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx9I8COrhIah"
      },
      "source": [
        "df3['story'] = df2['story'].apply(domination_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os8gMMFUvg_p",
        "outputId": "a96ed986-61c7-4f5a-ef98-d2ff772cfada"
      },
      "source": [
        "df3['story']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                    [4, 14, 16]\n",
              "1                [0, 18, 20, 22]\n",
              "2                 [4, 9, 11, 12]\n",
              "3                         [0, 6]\n",
              "4                  [5, 8, 9, 10]\n",
              "                 ...            \n",
              "95               [5, 12, 16, 17]\n",
              "96                   [0, 15, 17]\n",
              "97           [1, 13, 15, 16, 17]\n",
              "98                    [2, 7, 13]\n",
              "99    [7, 9, 13, 14, 17, 18, 20]\n",
              "Name: story, Length: 100, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NoZklTJokVp",
        "outputId": "9ed66497-0896-49f6-ebea-c21ec835050f"
      },
      "source": [
        "df3['story'].str.len().value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    32\n",
              "4    26\n",
              "3    23\n",
              "5     8\n",
              "1     6\n",
              "6     3\n",
              "7     2\n",
              "Name: story, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFML9-dVw9CU"
      },
      "source": [
        "file_path = '/content/gdrive/MyDrive/summarisation_dataset/tfidf_keywords_dataframe.pkl'\n",
        "df5 = pd.read_pickle(file_path)  # final df containing both hypothesised and final summaries  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBTw3npmvrTM"
      },
      "source": [
        "# convert the story column into actual summaries \n",
        "# wrap this into a function ?\n",
        "\n",
        "hypothesized_summaries = []  # making a new column for the df\n",
        "for i in range(len(df3)):\n",
        "  # for a row \n",
        "  hypothesized_summary = []  # list of sentences \n",
        "  for j in df3['story'][i]:\n",
        "    hypothesized_summary.append(df4['story'][i][j])  # jth element in the list of sentences \n",
        "  hypothesized_summaries.append(hypothesized_summary)\n",
        "\n",
        "df5['hypothesized summary'] = hypothesized_summaries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26E6HxZBkFyC"
      },
      "source": [
        "df5['story'] = df5['story'].apply(clean_sent_for_summary)  # sentence cleaning will also work for the entire paragraph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn60Lw4nyeVQ"
      },
      "source": [
        "df5.to_pickle('df_hypothesized_and_ref_summaries.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB42MDCD_UVC"
      },
      "source": [
        "# ROUGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cs6TfyR_VSr",
        "outputId": "d0cfc8b0-2fb2-407d-ff3e-e8da9d3bb34b"
      },
      "source": [
        "!pip install rouge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "vXk5fFrDo7Z8",
        "outputId": "43389492-2031-41da-e6d2-60d22429470f"
      },
      "source": [
        "df5.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>story</th>\n",
              "      <th>top_keywords</th>\n",
              "      <th>summmary</th>\n",
              "      <th>filename</th>\n",
              "      <th>hypothesized summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In the 17th century, Isaac Newton discovered t...</td>\n",
              "      <td>[color, spectrum, blue, indigo, newton, light,...</td>\n",
              "      <td>[[comparing newton's observation of prismatic ...</td>\n",
              "      <td>10.txt</td>\n",
              "      <td>Young was the first to measure the wavelengths...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>﻿A sea is a large body of salt water that is s...</td>\n",
              "      <td>[sea, ocean, water, land, produc, cycl, whale,...</td>\n",
              "      <td>[[owing to the present state of continental dr...</td>\n",
              "      <td>100.txt</td>\n",
              "      <td>However, population growth, industrialization,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>﻿Nazi Germany and the Third Reich (German: Dri...</td>\n",
              "      <td>[hitler, germani, reich, nazi, german, power, ...</td>\n",
              "      <td>[[nazi germany and the third reich (german: dr...</td>\n",
              "      <td>1.txt</td>\n",
              "      <td>Extensive public works were undertaken, includ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A rainbow is a meteorological phenomenon that ...</td>\n",
              "      <td>[rainbow, arc, droplet, refract, sky, inner, l...</td>\n",
              "      <td>[[rainbows can be full circles; however, the a...</td>\n",
              "      <td>11.txt</td>\n",
              "      <td>Rainbows caused by sunlight always appear in t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>﻿Shiva meaning \"The Auspicious One\", also know...</td>\n",
              "      <td>[shiva, god, hinduism, benevol, worship, depic...</td>\n",
              "      <td>[[he is the parabrahman within shaivism, one o...</td>\n",
              "      <td>26.txt</td>\n",
              "      <td>Shiva is also regarded as the patron god of yo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               story  ...                               hypothesized summary\n",
              "0  In the 17th century, Isaac Newton discovered t...  ...  Young was the first to measure the wavelengths...\n",
              "1  ﻿A sea is a large body of salt water that is s...  ...  However, population growth, industrialization,...\n",
              "2  ﻿Nazi Germany and the Third Reich (German: Dri...  ...  Extensive public works were undertaken, includ...\n",
              "3  A rainbow is a meteorological phenomenon that ...  ...  Rainbows caused by sunlight always appear in t...\n",
              "4  ﻿Shiva meaning \"The Auspicious One\", also know...  ...  Shiva is also regarded as the patron god of yo...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zah3RDfDoBWA",
        "outputId": "2f3da1f5-8b6c-4943-95dc-f43b82605bdc"
      },
      "source": [
        "df5['hypothesized summary'][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['More broadly, \"the sea\" is the interconnected system of Earth\\'s salty, oceanic waters—considered as one global ocean or as several principal oceanic divisions.',\n",
              " 'Atmospheric carbon dioxide is being absorbed in increasing amounts, lowering its pH in a process known as ocean acidification.',\n",
              " 'It is the scene of leisure activities including swimming, diving, surfing, and sailing.',\n",
              " 'However, population growth, industrialization, and intensive farming have all contributed to present-day marine pollution.']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FgEhMRovh6M",
        "outputId": "777dce11-1488-4fa5-9008-1dec3abc9244"
      },
      "source": [
        "type(df5['summmary'][1][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4Q9ul3NvrHJ",
        "outputId": "a0c3d6ea-f150-4ec4-90ee-f03f83b61066"
      },
      "source": [
        "df5['summmary'][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[owing to the present state of continental drift, the northern hemisphere is now fairly equally divided between land and sea (a ratio of about 2:3) but the south is overwhelmingly oceanic (1:4.7). salinity in the open ocean is generally in a narrow band around 3.5% by mass, although this can vary in more landlocked waters, near the mouths of large rivers, or at great depths., although the sea has been travelled and explored since prehistory, the modern scientific study of the sea—oceanography—dates broadly to the british challenger expedition of the 1870s. the sea is conventionally divided into up to five large oceanic sections—including the iho\\'s four named oceans (the atlantic, pacific, indian, and arctic) and the southern ocean; smaller, second-order sections, such as the mediterranean, are known as seas., oceanography has established that not all life is restricted to the sunlit surface waters: even under enormous depths and pressures, nutrients streaming from hydrothermal vents support their own unique ecosystem., this has also made it essential to warfare and left major cities exposed to earthquakes and volcanoes from nearby faults; powerful tsunami waves; and hurricanes, typhoons, and cyclones produced in the tropics., a sea is a large body of salt water that is surrounded in whole or in part by land. more broadly, \"the sea\" is the interconnected system of earth\\'s salty, oceanic waters—considered as one global ocean or as several principal oceanic divisions., whaling in the deep sea was once common but whales\\' dwindling numbers prompted international conservation efforts and finally a moratorium on most commercial hunting., this importance and duality has affected human culture, from early sea gods to the epic poetry of homer to the changes induced by the columbian exchange, from viking funerals to basho\\'s haikus to hyperrealist marine art, and inspiring music ranging from the shanties in the complaynt of scotland to rimsky-korsakov\\'s \"the sea and sinbad\\'s ship\" to a-mei\\'s \"listen to the sea\"., these nutrient-rich waters teem with life, which provide humans with substantial supplies of food—mainly fish, but also shellfish, mammals, and seaweed—which are both harvested in the wild and farmed., surface currents are formed by the friction of waves produced by the wind and by tides, the changes in local sea level produced by the gravity of the moon and sun.]']"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vZZsgE-xEPT"
      },
      "source": [
        "def clean_ref_sum(sum_list):\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR3XBnIyi5M3",
        "outputId": "0b1618c5-a2a3-4eb4-b8f4-588b25d61da6"
      },
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "rouge = Rouge()\n",
        "\n",
        "model_out = []\n",
        "reference = [\"nazi germany and the third reich (german: drittes reich) are common english names for the period of history in germany from 1933 to 1945, when it was a dictatorship under the control of adolf hitler and the nazi party (nsdap).under hitler's rule, germany was transformed into a fascist totalitarian state which controlled nearly all aspects of life.\"]\n",
        "\n",
        "rouge.get_scores(model_out, reference, avg=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge-1': {'f': 0.08450703769093457, 'p': 0.12, 'r': 0.06521739130434782},\n",
              " 'rouge-2': {'f': 0.02352940726920501,\n",
              "  'p': 0.034482758620689655,\n",
              "  'r': 0.017857142857142856},\n",
              " 'rouge-l': {'f': 0.08450703769093457, 'p': 0.12, 'r': 0.06521739130434782}}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoAVFd0yjDs0"
      },
      "source": [
        "\"f\" stands for f1_score, \"p\" stands for precision, \"r\" stands for recall."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFOVwjLei_k0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}